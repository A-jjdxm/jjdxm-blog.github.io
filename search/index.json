[{"content":"背景 从计算机发展的早期说起，单核处理器的性能遇到了物理限制，比如散热和时钟频率提升的困难，导致单核性能无法持续提升。这时候，多核处理器和并行计算开始兴起。摩尔定律虽然预测了晶体管数量的增长，但单线程性能不再显著提升，所以转向并行处理成为必然。\n科学计算、大数据、人工智能等领域对计算能力的需求激增，传统的串行计算无法满足这些需求，必须依靠并行处理来加速计算。例如，天气预报、基因测序，分子动力学模拟，物理仿真，大模型训练这些需要处理海量数据并需进行大量的数学计算的领域，并行计算能显著缩短计算时间。\n并行计算或称平行计算是相对于串行计算来说的。它是一种一次可执行多个指令的算法，目的是提高计算速度，及通过扩大问题求解规模，解决大型而复杂的计算问题。所谓并行计算可分为时间上的并行和空间上的并行。 时间上的并行就是指流水线技术，而空间上的并行则是指用多个处理器并发的执行计。\n说到这里我们把并行与并发做比较：\n并行(parallel)：指在同一时刻，有多条指令在多个处理器上同时执行。就好像两个人各拿一把铁锨在挖坑，一小时后，每人一个大坑。所以无论从微观还是从宏观来看，二者都是一起执行的。\n并发(concurrency)：指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，使多个进程快速交替的执行。这就好像两个人用同一把铁锨，轮流挖坑，一小时后，两个人各挖一个小一点的坑，要想挖两个大一点得坑，一定会用两个小时。\n并行计算 随着科技的持续发展，现在的计算机上的CPU和GPU都具有快速的计算能力。因此并行技术在CPU和GPU上都在迅速的发展。由于矩阵乘法中每个结果元素的计算都是相互独立的，因此从CPU和GPU的角度出发来说明并行技术在矩阵乘法上面的优势。\n串行计算 $$\r{c_{i,j}} = {a_{i1}}{n_{1j}} + {a_{i2}}{n_{2j}} + ...... + {a_{is}}{n_{sj}}. $$ 如下图。两个矩阵相乘必须满足A矩阵的列与B矩阵的行相等。\n那么在计算机上用C语言实现两个矩阵，这里就不多赘述了，只需要遍历两个矩阵的行和列并求和便可以计算出来，具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;math.h\u0026gt; #include \u0026lt;time.h\u0026gt; int main(int argc, char* argv[]) { int m, s, n; m = 1000; s = 1000; n = 1000; double* a = (double*)malloc(sizeof(double) * m * s); double* b = (double*)malloc(sizeof(double) * s * n); double* c = (double*)malloc(sizeof(double) * m * n); for (int i = 1; i \u0026lt;= m * s; i++) { a[i - 1] = 1;//(double)rand() / RAND_MAX; } for (int i = 1; i \u0026lt;= s * n; i++) { b[i - 1] = 1;//(double)rand() / RAND_MAX; } clock_t start, stop; start = clock(); for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { double t = 0; for (int k = 0; k \u0026lt; s; k++) { t += a[i * s + k] * b[k * n + j]; } c[i * n + j] = t; } } stop = clock(); for (int i = 0; i \u0026lt; m * n; i++) { printf(\u0026#34;%f \u0026#34;, c[i]); } printf(\u0026#34;\\n\u0026#34;); free(a); free(b); free(c); printf(\u0026#34;串行运行的时间为:%3fms\\n\u0026#34;, double(stop - start) / CLOCKS_PER_SEC*1000); } 上面算法设置的m，s，n的值均为1000，即为两个1000*1000的方阵相乘。其算法的时间复杂度为${O^3}$，最后的运行结果为\n串行运行的时间为:3220.000000ms\n在CPU上的并行(MPI+OPENMP) 环境配置\ncpu：R5-3500X(6核心6线程)睿频3.59GHz\n显卡：RTX 1660super 6G\n操作系统：windows10\n编程环境：visual studio2022\n多线程并行(OPENMP)\n单个物理核心通过分时复用技术（如Intel Hyper-Threading）模拟多个逻辑线程，共享核心的计算资源（如ALU、缓存），但独立维护线程的上下文。\n多线程并行可以通过C++的std::thread和python的threading来控制线程的创建与同步。OPENMP提供了编译器指令或API接口方便用户能够更快的进行多线程并行。在vs2022中只需要把项目设置中-\u0026gt;c/c++-\u0026gt;语言-\u0026gt;openmp支持改为是就配置好了OpenMp的环境。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 #include \u0026lt;omp.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;math.h\u0026gt; #include \u0026lt;time.h\u0026gt; static void matMultCPU_serial(const float*, const float*, float*, int); static void matMultCPU_parallel(const float*, const float*, float*, int); void genMat(float*, int); int main(int argc, char** argv) { /// test omp /// #pragma omp parallel for for (int i = 0; i \u0026lt; 10; i++) { printf(\u0026#34;Hello World %d from thread = %d\\n\u0026#34;, i, omp_get_thread_num()); } // Init matrix float* a, * b, * c, * d; int n = 1000; if (argc == 2) n = atoi(argv[1]); a = (float*)malloc(sizeof(float) * n * n); b = (float*)malloc(sizeof(float) * n * n); c = (float*)malloc(sizeof(float) * n * n); d = (float*)malloc(sizeof(float) * n * n); genMat(a, n); genMat(b, n); clock_t start, stop; start = clock(); ////// calculation code here /////// matMultCPU_serial(a, b, c, n); ////// end code /////// stop = clock(); printf(\u0026#34;CPU_Serial time: %3f ms\\n\u0026#34;, ((double)stop - start) / CLOCKS_PER_SEC * 1000.0); start = clock(); ////// calculation code here /////// matMultCPU_parallel(a, b, d, n); ////// end code /////// stop = clock(); printf(\u0026#34;CPU_Parallel time: %3f ms\\n\u0026#34;, ((double)stop - start) / CLOCKS_PER_SEC * 1000.0); return 0; } static void matMultCPU_serial(const float* a, const float* b, float* c, int n) { for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { double t = 0; for (int k = 0; k \u0026lt; n; k++) { t += (double)a[i * n + k] * b[k * n + j]; } c[i * n + j] = t; } } } static void matMultCPU_parallel(const float* a, const float* b, float* c, int n) { #pragma omp parallel for schedule(dynamic) for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { double t = 0; for (int k = 0; k \u0026lt; n; k++) { t += (double)a[i * n + k] * b[k * n + j]; } c[i * n + j] = t; } } } void genMat(float* arr, int n) { int i, j; for (i = 0; i \u0026lt; n; i++) { for (j = 0; j \u0026lt; n; j++) { arr[i * n + j] = (float)rand() / RAND_MAX + (float)rand() / (RAND_MAX * RAND_MAX); } } } OPENMP提供了标志并行区域的语句#pragma omp parallel for schedule(dynamic)可以使用多个线程并行执行该for语句。该语句会根据线程数量动态的分配for循环的任务。上述代码就是让每一条线程处理C矩阵的一小部分行中所有元素的计算任务实现并行计算。如需要了解其他的OPENMP的语句和功能可以参考OpenMP（使用C++多线程并行计算优化你的程序）入门篇 - 知乎。\n串行结果与并行结果性能对比如下：\nHello World 0 from thread = 0 Hello World 1 from thread = 0 Hello World 4 from thread = 2 Hello World 5 from thread = 2 Hello World 8 from thread = 4 Hello World 2 from thread = 1 Hello World 3 from thread = 1 Hello World 6 from thread = 3 Hello World 7 from thread = 3 Hello World 9 from thread = 5 CPU_Serial time: 3199.000000 ms CPU_Parallel time: 542.000000 ms\n多进程并行(MPI)\n现代CPU包含多个独立的物理核心（如4核、8核等），每个核心具备独立的运算单元（ALU）、寄存器、缓存等资源，能同时执行不同的指令流。\n与多线程并行不同的是多进程并行是利用CPU的多个核心或者分布式集群上的多台机器。但是多进程中的数据是互不共享的，如果需要数据传递则需要进程间的通信。那么为了方便多进程编程，MPI为在分布式内存架构下的进程间通信提供了规范和库支持。在程序的角度，MPI就是一系列函数接口，他们可以实现不同进程（不同内存区域）之间的消息传递。在windows环境下，笔者这里选择的是微软的MS-MPI配置MPI环境Downloads | MPICH，然后再vs2022中加入MPI的相关依赖Windows10+VisualStudio2019配置MPI 附在VS中直接运行MPI程序的方法_vs2019如何编译mpi程序-CSDN博客。最后，基于c语言的MPI程序是要通过mpiexec进行编译运行。运行命令为：F:\\MPI\\Bin\\mpiexec.exe -n 4 $(TargetPath)，-n 4表明并行的进程数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;mpi.h\u0026gt; #include\u0026lt;malloc.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; void printmarix(double* marix, int row,int col) { if (row == 0 || col==0) { printf(\u0026#34;请重新输入矩阵！！！\u0026#34;); } else { for (int i = 1; i \u0026lt;= row * col; i++) { printf(\u0026#34;%f \u0026#34;, marix[i - 1]); if (i % col == 0) { printf(\u0026#34;\\n\u0026#34;); } } } } int main(int argc, char* argv[]) { int myid, numprocs; double* A= NULL;//m*s double* B= NULL;//s*n double* AB= NULL;//m*n int m = 1000; int s = 1000; int n = 1000; int numsent = 0; double* buffer = (double*)malloc(sizeof(double) * s); if (buffer == NULL) { fprintf(stderr, \u0026#34;内存分配失败！\\n\u0026#34;); exit(EXIT_FAILURE); } A = (double*)malloc(sizeof(double) * m * s); if (A == NULL) { fprintf(stderr, \u0026#34;内存分配失败！\\n\u0026#34;); exit(EXIT_FAILURE); } B = (double*)malloc(sizeof(double) * s * n); if (B == NULL) { fprintf(stderr, \u0026#34;内存分配失败！\\n\u0026#34;); exit(EXIT_FAILURE); } AB = (double*)malloc(sizeof(double) * m * n); if (AB == NULL) { fprintf(stderr, \u0026#34;内存分配失败！\\n\u0026#34;); exit(EXIT_FAILURE); } clock_t start, stop; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;myid); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;numprocs); MPI_Status status; int master = 0;//设置主进程 if (myid == master) { //矩阵初始化数值 srand((unsigned int)time(NULL)); for (int i = 1; i \u0026lt;= m * s; i++) { A[i - 1] = 1;//(double)rand() / RAND_MAX; } for (int i = 1; i \u0026lt;= s * n; i++) { B[i - 1] = 1;//(double)rand() / RAND_MAX; } printf(\u0026#34;进程数量为%d\\n\u0026#34;, numprocs); start = clock(); MPI_Bcast(B, s*n , MPI_DOUBLE, master, MPI_COMM_WORLD); //将每一行的数据发送给各个进程 for (int i = 0; i \u0026lt; m; i++) { for (int k = 0; k \u0026lt; s; k++) { buffer[k] = A[i * s + k]; } int target = i % (numprocs-1)+1; MPI_Send(buffer, s, MPI_DOUBLE, target, i+1, MPI_COMM_WORLD); numsent = numsent + 1; } for (int i = 1; i \u0026lt; numprocs; i++) { int tmp = 1; MPI_Send(\u0026amp;tmp, 0, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);//发送结束标志 } double* ans = (double*)malloc(sizeof(double) * n); for (int i = 0; i \u0026lt; m; i++) { MPI_Recv(ans, n, MPI_DOUBLE, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, \u0026amp;status);//接收数据 int sender = status.MPI_SOURCE; for (int k = 0; k \u0026lt; n; k++) { AB[(status.MPI_TAG - 1) * n + k] = ans[k]; } } stop = clock(); } else { MPI_Bcast(B, s*n , MPI_DOUBLE, master, MPI_COMM_WORLD); while (1) { MPI_Recv(buffer, s, MPI_DOUBLE, master, MPI_ANY_TAG, MPI_COMM_WORLD, \u0026amp;status); if (status.MPI_TAG != 0) { int row = status.MPI_TAG; double* ans = (double*)malloc(sizeof(double) * n); for (int j = 0; j \u0026lt; n; j++) { double total = 0; for (int k = 0; k \u0026lt; s; k++) { total = total + buffer[k] * B[k * n + j]; } ans[j] = total; } MPI_Send(ans, n, MPI_DOUBLE,master, row, MPI_COMM_WORLD); } else { break; } } } if (myid == master) { printf(\u0026#34;A数组=\\n\u0026#34;); printmarix(A, m, s); printf(\u0026#34;B数组=\\n\u0026#34;); printmarix(B, s, n); printf(\u0026#34;AB数组=\\n\u0026#34;); printmarix(AB, m, n); printf(\u0026#34;MPI并行运行的时间为:%3fms\\n\u0026#34;, double(stop - start)/CLOCKS_PER_SEC*1000); free(A); free(B); free(AB); free(buffer); } MPI_Finalize(); return 0; } 在MPI程序设计中通过MPI_Init(\u0026amp;argc, \u0026amp;argv), MPI_Finalize() 来标志并行区域，每个进程都会执行这段并行区域的代码。上述代码中规定进程0用来初始化矩阵，打印消息，而从进程(1,2,3)用来计算矩阵的元素。由于各个进程之间的数据都是独立的，因此主进程需要把需要计算的数据发送到从进程去计算并且从进程计算好的结果需要发送到主进程中汇总打印输出。\ni % (numprocs-1)+1按行数平均分配任务到各个从进程中\nMPI_Send，MPI_Recv发送数据到其他进程，接收其他进程发送的数据\n虽然运行的进程数是4，但是实际参与计算的进程数只有3。最后运行的性能结果如下：\nMPI并行运行的时间为:1493.000000ms\n在GPU上的并行(CUDA) 可以看到GPU包括更多的运算核心，其特别适合数据并行的计算密集型任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。另外，CPU上的线程是重量级的，上下文切换开销大，但是GPU由于存在很多核心，其线程是轻量级的。因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效\nCUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语言的支持，如C/C++，Python，Fortran等语言，这里我们选择CUDA C/C++接口对CUDA编程进行讲解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 #include \u0026#34;cuda_runtime.h\u0026#34; #include \u0026#34;device_launch_parameters.h\u0026#34; #include \u0026#34;cuda.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;math.h\u0026gt; #include \u0026lt;omp.h\u0026gt; #define MATRIX_SIZE 1000 #define BLOCK_SIZE 16 int DevicedChoosed = 0; void printDeviceProp(const cudaDeviceProp\u0026amp; prop) { printf(\u0026#34;Device Name : %s.\\n\u0026#34;, prop.name); printf(\u0026#34;totalGlobalMem : %d.\\n\u0026#34;, prop.totalGlobalMem); printf(\u0026#34;sharedMemPerBlock : %d.\\n\u0026#34;, prop.sharedMemPerBlock); printf(\u0026#34;regsPerBlock : %d.\\n\u0026#34;, prop.regsPerBlock); printf(\u0026#34;warpSize : %d.\\n\u0026#34;, prop.warpSize); printf(\u0026#34;memPitch : %d.\\n\u0026#34;, prop.memPitch); printf(\u0026#34;maxThreadsPerBlock : %d.\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;maxThreadsDim[0 - 2] : %d %d %d.\\n\u0026#34;, prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]); printf(\u0026#34;maxGridSize[0 - 2] : %d %d %d.\\n\u0026#34;, prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]); printf(\u0026#34;totalConstMem : %d.\\n\u0026#34;, prop.totalConstMem); printf(\u0026#34;major.minor : %d.%d.\\n\u0026#34;, prop.major, prop.minor); printf(\u0026#34;clockRate : %d.\\n\u0026#34;, prop.clockRate); printf(\u0026#34;textureAlignment : %d.\\n\u0026#34;, prop.textureAlignment); printf(\u0026#34;deviceOverlap : %d.\\n\u0026#34;, prop.deviceOverlap); printf(\u0026#34;multiProcessorCount : %d.\\n\u0026#34;, prop.multiProcessorCount); } //CUDA 初始化 bool InitCUDA() { int count; //取得支持Cuda的装置的数目 cudaGetDeviceCount(\u0026amp;count); if (count == 0) { fprintf(stderr, \u0026#34;There is no device.\\n\u0026#34;); return false; } int i; for (i = 0; i \u0026lt; count; i++) { cudaDeviceProp prop; cudaGetDeviceProperties(\u0026amp;prop, i); //打印设备信息 printDeviceProp(prop); if (cudaGetDeviceProperties(\u0026amp;prop, i) == cudaSuccess) { if (prop.major \u0026gt;= 1) { break; } } } if (i == count) { fprintf(stderr, \u0026#34;There is no device supporting CUDA 1.x.\\n\u0026#34;); return false; } cudaSetDevice(i); DevicedChoosed = i; return true; } void matMultCPU(const float* a, const float* b, float* c, int n) { #pragma omp parallel for for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { double t = 0; for (int k = 0; k \u0026lt; n; k++) { t += (double)a[i * n + k] * b[k * n + j]; } c[i * n + j] = t; } } } //GPU并行计算矩阵乘法 __global__ void matMultCUDAKernel1(const float* a, const float* b, float* c, int n) { //计算这个 thread 应该计算的 row 和 col const int col = blockIdx.x * blockDim.x + threadIdx.x; const int row = blockIdx.y * blockDim.y + threadIdx.y; int i; //计算矩阵乘法 Kahan’s Summation Formula if (row \u0026lt; n \u0026amp;\u0026amp; col \u0026lt; n) { float t = 0; float y = 0; for (i = 0; i \u0026lt; n; i++) { float r; y -= a[row * n + i] * b[i * n + col]; r = t - y; y = (r - t) + y; t = r; } c[row * n + col] = t; } } void genMat(float* arr, int n) { int i, j; for (i = 0; i \u0026lt; n; i++) { for (j = 0; j \u0026lt; n; j++) { arr[i * n + j] = (float)rand() / RAND_MAX + (float)rand() / (RAND_MAX * RAND_MAX); } } } typedef struct Error { float max; float average; }Error; Error accuracyCheck(const float* a, const float* b, int n) { Error err; err.max = 0; err.average = 0; for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { if (b[i * n + j] != 0) { //fabs求浮点数x的绝对值 float delta = fabs((a[i * n + j] - b[i * n + j]) / b[i * n + j]); if (err.max \u0026lt; delta) err.max = delta; err.average += delta; } } } err.average = err.average / (n * n); return err; } int main(int argc, char** argv) { //CUDA 初始化 if (!InitCUDA()) return 0; cudaDeviceProp prop; cudaGetDeviceProperties(\u0026amp;prop, DevicedChoosed); //定义矩阵 float* a, * b, * c, * d; int n = MATRIX_SIZE; if (argc \u0026gt;= 2) n = atoi(argv[1]) \u0026gt; 0 ? atoi(argv[1]) : MATRIX_SIZE; //分配host内存 cudaMallocHost((void**)\u0026amp;a, sizeof(float) * n * n); cudaMallocHost((void**)\u0026amp;b, sizeof(float) * n * n); cudaMallocHost((void**)\u0026amp;c, sizeof(float) * n * n); d = (float*)malloc(sizeof(float) * n * n); genMat(a, n); genMat(b, n); float* cuda_a, * cuda_b, * cuda_c; clock_t start, stop; //分配GPU上的内存 cudaMalloc((void**)\u0026amp;cuda_a, sizeof(float) * n * n); cudaMalloc((void**)\u0026amp;cuda_b, sizeof(float) * n * n); cudaMalloc((void**)\u0026amp;cuda_c, sizeof(float) * n * n); //拷贝数据至GPU内存 cudaMemcpy(cuda_a, a, sizeof(float) * n * n, cudaMemcpyHostToDevice); cudaMemcpy(cuda_b, b, sizeof(float) * n * n, cudaMemcpyHostToDevice); start = clock(); //调用核函数计算 dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE, 1); dim3 gridSize((n + BLOCK_SIZE - 1) / BLOCK_SIZE, (n + BLOCK_SIZE - 1) / BLOCK_SIZE, 1); matMultCUDAKernel1 \u0026lt;\u0026lt; \u0026lt;gridSize, blockSize \u0026gt;\u0026gt; \u0026gt; (cuda_a, cuda_b, cuda_c, n); //计算结果复制回主存，隐式调用同步函数 cudaMemcpy(c, cuda_c, sizeof(float) * n * n, cudaMemcpyDeviceToHost); stop = clock(); //释放GPU上的内存 cudaFree(cuda_a); cudaFree(cuda_b); cudaFree(cuda_c); //GPU memory printf(\u0026#34;GPU memory: %e MB\\n\u0026#34;, (double)(n * n * 8) / (1024. * 1024.)); //GPU time printf(\u0026#34;GPU time: %3f ms\\n\u0026#34;, (double)(stop - start) / CLOCKS_PER_SEC * 1000.0); //CPU time start = clock(); matMultCPU(a, b, d, n); stop = clock(); printf(\u0026#34;CPU time: %3f ms\\n\u0026#34;, (double)(stop - start) / CLOCKS_PER_SEC * 1000.0); //精度检测 Error error; error = accuracyCheck(c, d, n); printf(\u0026#34;Max error: %g Average error: %g\\n\u0026#34;, error.max, error.average); return 0; } cudaMallocHost，cudaMalloc分别表示在在CPU的分配内存，和在CPU分配内存\ncudaMemcpy可以从CPU的内存数据拷贝到GPU的内存中，或者拷贝GPU的内存的数据到CPU内存中\nkernel是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用__global__符号声明，在调用时需要用matMultCUDAKernel1\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;来指定kernel要执行的线程数量，在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量threadIdx来获得。\n__global__：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步\n__device__：在device上执行，单仅可以从device中调用，不可以和__global__同时用\n__host__：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译\ndim 用力定义GPU的gird大小和block大小\nmatMultCUDAKernel1核函数中通过 blockIdx ， threadIdx，blockDim确定某个线程的二维索引，这与举证相乘正好对应。因此可以用GPU一个线程的对应的二维索引(row，col)处理对应的A矩阵的第row行与B矩阵的第col列相乘。这个线性计算完成后把计算结果放在C矩阵中的第row行第col列。最后输出结果如下：\nDevice Name : NVIDIA GeForce GTX 1660 SUPER. totalGlobalMem : 2147024896. sharedMemPerBlock : 49152. regsPerBlock : 65536. warpSize : 32. memPitch : 2147483647. maxThreadsPerBlock : 1024. maxThreadsDim[0 - 2] : 1024 1024 64. maxGridSize[0 - 2] : 2147483647 65535 65535. totalConstMem : 65536. major.minor : 7.5. clockRate : 1830000. textureAlignment : 512. deviceOverlap : 1. multiProcessorCount : 22. GPU memory: 7.629395e+00 MB GPU time: 43.000000 ms CPU time: 3337.000000 ms Max error: 1.19209e-07 Average error: 1.1436e-09\n","date":"2025-02-27T00:00:00Z","image":"https://www.jjdxm.cn/p/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98/matrix_hu10136494627670933984.jpg","permalink":"https://www.jjdxm.cn/p/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98/","title":"并行计算-矩阵相乘"},{"content":"前言 本文主要介绍关于超算的架构，以及如何调度超算中的计算资源。在资源调度的过程中会涉及到一些Linux命令，\n后续慢慢补充\n","date":"2025-02-26T00:00:00Z","image":"https://www.jjdxm.cn/p/%E8%B6%85%E7%AE%97%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6linux/%E8%B6%85%E7%AE%97_hu2675433159711362526.png","permalink":"https://www.jjdxm.cn/p/%E8%B6%85%E7%AE%97%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6linux/","title":"超算资源调度（Linux）"},{"content":"背景 笔者第一次写学术论文，其研究方向是弹性光网络，本次投递的期刊是JLT。在后续调整论文格式时候使用LATEX来写。而本文主要描述了在使用LATEX写论文时候，一些论文注意细节和LATEX的使用用法。以便帮助后续学弟和学妹们更快的完成论文的撰写。\n准备工作 LATREX下载 【LaTex】LaTex的下载与安装（2024新手小白超详细、超简洁 Windows系统）_latex下载-CSDN博客\n安装完成后会有下面两个软件：\ntexlive: LATEX本体，负责编译.tex文件，并生成对应的.pdf文件\nTexStudio： 编写.tex文件的编辑器(类似于代码编辑，还有许多其他的编辑器可以自行下载如Overleaf, 在线LaTeX编辑器)。\n论文LATEX模板下载(以IEEEtrans为例) IEEE-Template Selector\nIEEE里面的期刊通过上述链接选择对应的期刊下载LATEX模板。如果不是，则百度对应的期刊名到官网下载对应的LATEX模板。\n这里我选择的是Journal of Lightwave Technology期刊的LATEX模板IEEEtrans。下载内容如下图：\nIEEEtran.cls提供给模板的接口，bare_jrnl_new_sample4.tex是IEEEtrans对应的期刊的latex模板，点击打开bare_jrnl_new_sample4.tex并选择pdfLaTex编译运行，则会输出对应的pdf。(笔者是用latex自带的Texwork打开的，可以用其它编辑器打开)\nLATEX论文撰写 一篇论文的结构在模板里面已经非常清晰了，文件里面的头文件基本上不用变化。只需找到模板中对应的代码段(title, abstract, author, IEEEkeywords, section, subsection, \u0026hellip;)把内容填进去。如果有什么不懂的可以百度或者查看目录下面的New_IEEEtran_how-to.pdf帮助文档，笔者这里主要介绍一些需要自定义并且可能会出现问题的地方。\n图片 为了方便管理图片，这里我这里把整篇论文要用到的图片都放在模板路径下面的figures文件夹里，然后再模板文件中**\\begin{document}上一行加入\\graphicspath{{figures/}}**，这样就规定了图片的路径。\n要在论文中插入一张图片代码如下：\n1 2 3 4 5 6 \\begin{figure}[t] \\centering \\includegraphics[width=0.5\\textwidth]{fig1.pdf} \\caption{Co-propagation and Counter-propagation} \\label{fig1} \\end{figure} [t]：表示将当前图片放置在当前页面的顶部(h当前位置，b表示底部\u0026hellip;)\n\\centering：表示图片居中\nwidth=0.5\\textwidth：表示设置图片的宽度为半个页面宽度(注：这里没有设置高度所以会保持长宽的比例)\nfig1.pdf：表示插入的图片文件名，上面已经设置好图片的路径，所以这里直接填入文件名就行\n\\caption：设置图片的标题\n\\label{fig1}：设置图片的唯一标志，为了方便文中引用该图片。这里笔者设置的名称是fig1，使用\\ref{fig1}便可以引用\n要在论文中插入多张图片代码如下(竖排)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \\begin{figure}[!t] \\centering \\subfloat[Performance comparisons in NSFNET]{ \\includegraphics[width=0.4\\textwidth]{gN1A.pdf}% \\label{BBPa} } \\hfil \\subfloat[Performance comparisons in COST239]{ \\includegraphics[width=0.4\\textwidth]{gN2A.pdf}% \\label{BBPb} } \\caption{Performance comparisons on blocking probability among the conventional PCMSA, BI-PCMSA, and BI-CI schemes and the BCSRA scheme in (a) NSFNET and (b) COST239 network.} \\label{BBP} \\end{figure} ！t:系统忽略“美学”标准，放置在顶部(这里加不加！感觉没有上面区别)\n\\hfil：填充两张图片中间的空白\n\\subfloat：设置子图\n要在论文中插入多张图片代码如下(横排)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \\begin{figure*}[!t] \\centering \\subfloat[]{\\includegraphics[width=2.5in]{fig1.png}% \\label{fig_first_case}} \\hfil \\subfloat[]{\\includegraphics[width=2.5in]{fig1.png}% \\label{fig_second_case}} \\\\ \\subfloat[]{\\includegraphics[width=2.5in]{fig1.png}% \\label{fig_first_case}} \\hfil \\subfloat[]{\\includegraphics[width=2.5in]{fig1.png}% \\label{fig_second_case}} \\caption{Dae. Ad quatur autat ut porepel itemoles dolor autem fuga. Bus quia con nessunti as remo di quatus non perum que nimus. (a) Case I. (b) Case II.} \\label{fig_sim} \\end{figure*} \\begin{figure}*：设置跨栏的图片\n\\hfil：填充两张图片中间的空白\n\\\\：图片换行\n表格 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \\begin{table}[!b] \\begin{center} \\caption{Flow of Lightpath Requests in Fig. \\ref{fig2}(a)} \\label{tab2} \\renewcommand{\\arraystretch}{1.5} \\setlength{\\tabcolsep}{8pt} \\resizebox{0.5\\textwidth}{!}{ \\begin{tabular}{| c | c | c | c | c | c | c |} \\hline request \u0026amp; route \u0026amp; e1 \u0026amp; e2 \u0026amp; e3 \u0026amp; e4 \u0026amp; e5\\\\ \\hline $R_1$ \u0026amp; 1-2-4 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0\\\\ \\hline $R_2$ \u0026amp; 2-3-1 \u0026amp; 0 \u0026amp; -1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0\\\\ \\hline $R_3$ \u0026amp; 4-3 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; -1\\\\ \\hline \\end{tabular} } \\end{center} \\end{table} \\begin{table}[!b]：设置表格为页面底部 \\begin{center}：表格居中\n\\caption：表格标题\n\\renewcommand{\\arraystretch}{1.5}：设置表格的行距为1.5倍\n\\setlength{\\tabcolsep}{8pt}：设置表格的列宽\n\\resizebox{0.5\\textwidth}{!}：设置表格宽的大小为半个页面大小，并保持长宽比例\n\\begin{tabular}：设置表格的形式，上述代码“|”设置表格的竖线(c表示单元格内水平对齐，l表示单元格左对齐\u0026hellip;, )，“\\hline”设置表格的横线。然后将内容填入表格，”\u0026amp;“表示水平下一列，”\\\\“表示下一行\n如果想单独设置表格某一列的列宽可以使用p{xx}，下面就是设置第二列的列宽为6cm\n1 \\begin{tabular}{| l | p{6cm}|} 算法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \\begin{algorithm}[!b] \\caption{Analysis of bidirectional spectrum resource.}\\label{alg:alg2} \\renewcommand{\\algorithmicrequire}{\\textbf{Input:}} \\renewcommand{\\algorithmicensure}{\\textbf{Output:}} \\begin{algorithmic}[1] \\REQUIRE The core $C$ and frequency slot position $f$ of the lightpath, and the link information of the request, information of the request \\ENSURE $Flag$ and $CIFA$ \\STATE $Flag=0, CI=0, free\\_slot=0$. \\STATE Temporarily modify the link information of the current lightpath. \\FOR {each $e \\in l_r^{temp}$} \\FOR {$k = 1:F{S_r}$} \\STATE $free\\_slot$=$free\\_slot+|\\alpha _{{c_i}}^{f + k}|$ \\FOR {each ${c_{ac}} \\in AC{S_c}$} \\IF { $\\eta _{c,{c_{ac}}}^{f + k} = = 2$} \\STATE \\textbf{return} \\ELSIF {$\\eta _{c,{c_{ac}}}^{f + k} = = 1$} \\STATE $CI=CI+1$ \\ELSIF {$\\eta _{c,{c_{ac}}}^{f + k} = = 0$} \\STATE \\textbf{continue} \\ENDIF \\ENDFOR \\ENDFOR \\IF {$free\\_slot = = 0$} \\STATE Estimate the value of fragmentation and $CIFA$ metric according to \\eqref{eq4}\\eqref{eq6}\\eqref{eq7}. \\ELSE \\STATE \\textbf{return} \\ENDIF \\ENDFOR \\STATE $Flag=1$ \\STATE \\textbf{return} $Flag, CIFA$ \\end{algorithmic} \\label{alg2} \\end{algorithm} \\begin{algorithm}[!b]：设置算法位置为底部\n\\caption：设置算法标题\n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}：将require换成input \\renewcommand{\\algorithmicensure}{\\textbf{Output:}}：将ensure换成output\n\\begin{algorithmic}[1]：算法开始，并从1开始标号\n\\REQUIRE：设置输入参数\n\\ENSURE：设置输出参数\n\\FOR \\ENDFOR \\IF \\ELSE \\ENDIF：与写伪代码一样\n\\STATE：设置这一行的内容\n\\label{alg2}：算法的标签，方便引用\\ref{alg2}\n列表 列表这一块的话\n无序列表：\n1 2 3 4 5 6 7 \\begin{list}{}{} \\item{bare\\_jrnl.tex} \\item{bare\\_conf.tex} \\item{bare\\_jrnl\\_compsoc.tex} \\item{bare\\_conf\\_compsoc.tex} \\item{bare\\_jrnl\\_comsoc.tex} \\end{list} 有序列表：\n1 2 3 4 5 6 7 \\begin{enumerate} \\item{bare\\_jrnl.tex} \\item{bare\\_conf.tex} \\item{bare\\_jrnl\\_compsoc.tex} \\item{bare\\_conf\\_compsoc.tex} \\item{bare\\_jrnl\\_comsoc.tex} \\end{enumerate} 符号列表：\n1 2 3 4 5 6 7 8 \\subsubsection*{\\bf A simple bulleted list} \\begin{itemize} \\item{bare\\_jrnl.tex} \\item{bare\\_conf.tex} \\item{bare\\_jrnl\\_compsoc.tex} \\item{bare\\_conf\\_compsoc.tex} \\item{bare\\_jrnl\\_comsoc.tex} \\end{itemize} 公式 文章中的正文中出现的公式可以用**$F{S_{{r_i}}} = \\left[ {\\frac{{{w_i}}}{{{C_{slot}} \\cdot m}}} \\right] + GB$**，\n需要标号的公式可以用如下句柄：\n1 2 3 4 \\begin{equation} \\label{eq1} F{S_{{r_i}}} = \\left[ {\\frac{{{w_i}}}{{{C_{slot}} \\cdot m}}} \\right] + GB, \\end{equation} 后续文中需要引用公式的时候可以使用\\eqref{eq1}来引用该公式的标号。 如果公式已经在mathtype中编辑好那么可以直接复制公式到tex文件中。\nmathtype -\u0026gt; 预置 -\u0026gt;剪切与复制预置 -\u0026gt;选择LaTex\n\\[F{S_{{r_i}}} = \\left[ {\\frac{{{w_i}}}{{{C_{slot}} \\cdot m}}} \\right] + GB\\] 这样复制过来的直接就是latex公式代码了！！！（记得把[ ]去掉）\n参考文献 笔者第一次在弄论文的参考文献时踩了很大的坑。我们知道论文的参考文献要满足以下几个条件（以Journal of Lightwave Technology为例）：\n参考文献的格式要符合从作者-标题-期刊名-页数-年月份 参考文献中引用文献的期刊名要保持斜体和缩写 参考文献中引用文献的作者数量太长的话要缩写或者简写 文章中的引用文献的序号需要要保持从大到小 我们知道设置参考文献的方式有两种\nbibitem：\n想如下的代码形式，使用\\bibitem直接在文中把参考文献的格式一一调整好包括作者姓名，期刊名要缩写和斜体。\n1 2 3 4 5 6 7 \\begin{thebibliography}{10} \\bibliographystyle{IEEEtran} \\bibitem{Bao2023} B. Bao, H. Yang, Q. Yao, L. Guan, J. Zhang, and M. Cheriet, “Resource Allocation With Edge-Cloud Collaborative Traffic Prediction in Integrated Radio and Optical Networks,” {\\it{IEEE Access}}, vol. 11, pp. 7067–7077, 2023. \\bibitem{Shao2023} J. Shao, S. Zhang, W. Sun, and W. Hu, “Dynamic frequency slot allocation on IP-over-EON access links with multiple-type and time-varying traffic,” {\\it{J. Opt. Commun. Netw.}}, vol. 15, no. 6, pp. 304–317, 2023, doi: 10.1364/JOCN.481355. \\end{thebibliography} {10}表示序号需要的宽度，统一使用两位数的宽度序号。\n\\bibliographystyle{IEEEtran}表示参考文献风格\n{Bao2023}表示该文献的标签并且在所有文献中必须是唯一的，引用时使用\\cite{Bao2023}\n所有文献的形式编辑好以后直接运行就行，参考文献的顺序会按照编辑好的顺序排序。\nbibtex：\n这种方法需要建立参考文献数据库，引用的时候调用所需要的参考文献。每篇文献有它自己的引用的bib文件(可以在网上下载)。下面就是包含两篇文章的文献数据库（是不是有点想json文件的形式，属性\u0026ndash;字段，python里面也叫做字典）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @ARTICLE{9611289, author={Zhang, Juan and Bao, Bowen and Yao, Qiuyan and Ren, Danping and Hu, Jinhua and Zhao, Jijun}, journal={IEEE Access}, title={Resource Allocation Scheme Based on Complete Planning Process for Immediate and Advance Reservation in SDM-EONs}, year={2021}, volume={9}, number={}, pages={153575-153585}, keywords={Resource management;Planning;Optical fiber networks;WDM networks;Routing;Optimization;Quality of service;Advanced reservation (AR);blocking probability;complete planning process;elastic optical networks (EONs);immediate reservation (IR);space division multiplexing (SDM)}, doi={10.1109/ACCESS.2021.3127317} } @article{Ma2023, author = {Ma, Yudong and Yang, Xin and Sun, Qang and Zhao, Yue}, doi = {10.1364/JOCN.496417}, issn = {19430639}, journal = {Journal of Optical Communications and Networking}, number = {9}, pages = {687--699}, title = {{Dynamic resource allocation for multicast in SDM-EON: time-decoupled dynamic path cross talk and joint weight}}, volume = {15}, year = {2023} } 把所有文献的bib文件整合成一个文献数据库并放在模板文件同级目录下，在参考文献的代码部分加上\\bibliography{bibfile}就可以调用对应的bibfile.bib文献数据库，当在文档中想使用引用时, 插入 LaTeX 命令\\cite{9611289，Ma2023}。参考文献的排序会按照引用的先后顺序排列好。\n最后\n用LaTeX编译你的 .tex 文件 , 这是生成一个 .aux 的文件, 这告诉 BibTeX 将使用那些应用； 用BibTeX 编译 .bib 文件； 再次用LaTeX 编译你的 .tex 文件，这个时候在文档中已经包含了参考文献，但此时引用的编号可能不正确； 最后用 LaTeX 编译你的 .tex 文件，如果一切顺利的话, 这是所有东西都已正常了. 在理解上述的两种方法之后，笔者便思考两者哪一个能更快的实现上述的要求。思考一番后，第二种方法看起来确实是比较便捷。因为这种方法不仅不需要手动去调整参考文献的格式(自动生成IEEEtrans的风格)，而且参考文献的排序会按照引用的先后顺序排列好。\n为了快速的收集bib文件，笔者选择了一款文献管理工具mendeley.com/autoupdates/installers/1.19.5，只需把参考文献导入mendeley，它能够快速的导出所有文献的bib文件并直接生成文献数据库。它也可以查看对应期刊的参考文献风格。\n最后出现以下结果！！！\n可以看到图中生成的参考文献并没有满足我说的要求，在看了一下bib文件中的内容之后，发现bibtex生成的参考文献是根据bib中每个文献对应的属性字段来生成的。而有些文章的属性字段缺失，作者也不能保证缩写和简写，并且期刊名也不能缩写。这可能需要对每一篇文章的bib内容调整。笔者一直也在寻找解决的办法，导致浪费了大量的时间。如果您有更好的解决方案，欢迎与笔者联系！\n在与导师商量的之后，最后决定用第一种方法生成参考文献\n第一种方法看似内容和格式也要自己一个一个的敲上去，但是笔者在用mendeley的时候可以查看IEEEtrans的风格的参考文献，**发现格式非常统一和正确！！！**于是直接从mendeley把参考文献的内容(Formatted Citation)直接导入来并粘贴到模板中就行了（不是bib文件）\n但是还有一个难题就是，刚刚说了第一种方法参考文献生成的时候是按照编辑的顺序排列的，文章中引用的参考文献的需要顺序并没有按照从小到大，所以说我们还需要对这些bibitem调整顺序以便确保文中的引用文献的序号需要从小到大。\n这里介绍一个使用LaTeX的bibitems引用时按照引用顺序对文献进行编号 - 脂环 - 博客园排序方法能根据引用的顺序把参考文献调整好。最后生成的参考文献的结果要检查一下是否格式正确！\n总结 最后，本文主要对在使用latex去撰写论文时如何自定义自己的公式，图片，表格，算法，列表，和参考文献提供了笔者所使用的方法和一些细节并给出了一些建议。由于笔者也是第一次使用latex撰写论文，每个细节都是自己上网或者AI搜索解决的，一些方面的方法和细节无法做到最优。如果读者有更好的想法或者有什么问题欢迎与我讨论。(qq: 1693592330)\n","date":"2025-02-18T00:00:00Z","image":"https://www.jjdxm.cn/p/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87latex--%E4%BB%A5ieeetrans%E4%B8%BA%E4%BE%8B%E5%AD%90/IEEE_hu669125944197684707.png","permalink":"https://www.jjdxm.cn/p/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87latex--%E4%BB%A5ieeetrans%E4%B8%BA%E4%BE%8B%E5%AD%90/","title":"学术论文latex--以IEEEtrans为例子"},{"content":"在漫长的学习和工作之路中，难免会有一些技术问题和心得。正所谓好记性不如烂笔头。在朋友的介绍hugo后可以快速便捷的搭建自己的静态网站。于是笔者决定搭建自己的博客网站。\n本博客网站是基于hugo生成的\n快速静态网站生成器：Hugo 能够将内容转换为 HTML 文件，与动态网站生成器不同，Hugo 在创建内容时就完成页面的构建，而不是每次用户请求时都动态生成页面。 跨平台运行：Hugo 可以在多种操作系统上运行，包括 Mac OS X、Linux、Windows 等。 主题支持：Hugo 支持完整的主题系统，允许用户通过更换主题来改变网站的外观。 内容管理：Hugo 支持 Markdown 语法，使得内容编写变得简单，同时也支持 TOML、YAML 和 JSON 格式的元数据。 部署灵活性：Hugo 生成的网站可以部署在任何平台上，包括 Heroku、GoDaddy、DreamHost、GitHub Pages、Google Cloud Storage、Amazon S3 和 CloudFront 等，并且与 CDN 良好协作。 无依赖：Hugo 网站不需要依赖昂贵的运行时环境，如 Ruby、Python 或 PHP，也不需要数据库。 本网站选用的主题是dream\n快来搭建自己的博客网站！！！\n","date":"2024-12-05T00:00:00Z","image":"https://www.jjdxm.cn/p/hugo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/hugo_hu381683957164872731.png","permalink":"https://www.jjdxm.cn/p/hugo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/","title":"hugo博客网站"}]